{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save all rainfall results\n",
    "\n",
    "from __future__ import print_function\n",
    "import torch\n",
    "import matplotlib as plt\n",
    "import argparse\n",
    "import sys\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torchvision import datasets, models, transforms\n",
    "import platform\n",
    "from datetime import timedelta, date, datetime\n",
    "from model import vdsr_dem\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from math import log10\n",
    "import time\n",
    "from PrepareData import ACCESS_BARRA_v2_0,ACCESS_BARRA_v2_1,ACCESS_BARRA_vdsr,ACCESS_BARRA_vdsr_pr_dem\n",
    "import tqdm\n",
    "from statsmodels.distributions.empirical_distribution import ECDF\n",
    "# ===========================================================\n",
    "# Training settings\n",
    "# ===========================================================\n",
    "parser = argparse.ArgumentParser(description='PyTorch Super Res Example')\n",
    "# Hardware specifications\n",
    "parser.add_argument('--n_threads', type=int, default=0,\n",
    "                    help='number of threads for data loading')\n",
    "\n",
    "parser.add_argument('--cpu', action='store_true',help='cpu only?') \n",
    "\n",
    "# hyper-parameters\n",
    "parser.add_argument('--train_name', type=str, default=\"vdsr_pr\", help='training name')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int, default=44, help='training batch size')\n",
    "parser.add_argument('--testBatchSize', type=int, default=4, help='testing batch size')\n",
    "parser.add_argument('--nEpochs', type=int, default=200, help='number of epochs to train for')\n",
    "parser.add_argument('--lr', type=float, default=0.0001, help='Learning Rate. Default=0.01')\n",
    "parser.add_argument('--seed', type=int, default=123, help='random seed to use. Default=123')\n",
    "\n",
    "# model configuration\n",
    "parser.add_argument('--upscale_factor', '-uf',  type=int, default=4, help=\"super resolution upscale factor\")\n",
    "parser.add_argument('--model', '-m', type=str, default='vdsr', help='choose which model is going to use')\n",
    "\n",
    "#data\n",
    "parser.add_argument('--pr', type=bool, default=True,help='add-on pr?')\n",
    "\n",
    "parser.add_argument('--train_start_time', type=type(datetime(1990,1,25)), default=datetime(1990,1,2),help='r?')\n",
    "parser.add_argument('--train_end_time', type=type(datetime(1990,1,25)), default=datetime(1990,2,9),help='?')\n",
    "parser.add_argument('--test_start_time', type=type(datetime(2012,1,1)), default=datetime(2012,1,1),help='a?')\n",
    "parser.add_argument('--test_end_time', type=type(datetime(2012,12,31)), default=datetime(2012,12,31),help='')\n",
    "\n",
    "parser.add_argument('--dem', action='store_true',help='add-on dem?') \n",
    "parser.add_argument('--psl', action='store_true',help='add-on psl?') \n",
    "parser.add_argument('--zg', action='store_true',help='add-on zg?') \n",
    "parser.add_argument('--tasmax', action='store_true',help='add-on tasmax?') \n",
    "parser.add_argument('--tasmin', action='store_true',help='add-on tasmin?')\n",
    "parser.add_argument('--leading_time_we_use', type=int,default=1\n",
    "                    ,help='add-on tasmin?')\n",
    "parser.add_argument('--ensemble', type=int, default=11,help='total ensambles is 11') \n",
    "parser.add_argument('--channels', type=float, default=0,help='channel of data_input must') \n",
    "#[111.85, 155.875, -44.35, -9.975]\n",
    "parser.add_argument('--domain', type=list, default=[112.9, 154.25, -43.7425, -9.0],help='dataset directory')\n",
    "\n",
    "parser.add_argument('--file_ACCESS_dir', type=str, default=\"../data/\",help='dataset directory')\n",
    "parser.add_argument('--file_BARRA_dir', type=str, default=\"../data/barra_aus/\",help='dataset directory')\n",
    "parser.add_argument('--file_DEM_dir', type=str, default=\"../DEM/\",help='dataset directory')\n",
    "parser.add_argument('--precision', type=str, default='single',choices=('single', 'half','double'),help='FP precision for test (single | half)')\n",
    "\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "#pr_dem\n",
    "def write_log(log):\n",
    "    print(log)\n",
    "    if not os.path.exists(\"./save/\"+args.train_name+\"/\"):\n",
    "        os.mkdir(\"./save/\"+args.train_name+\"/\")\n",
    "    my_log_file=open(\"./save/\"+args.train_name + '/train.txt', 'a')\n",
    "#     log=\"Train for batch %d,data loading time cost %f s\"%(batch,start-time.time())\n",
    "    my_log_file.write(log + '\\n')\n",
    "    my_log_file.close()\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluation(net,val_dataloders,loss,criterion):\n",
    "    net.eval()\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    test_loss=0\n",
    "    avg_psnr = 0\n",
    "    start=time.time()\n",
    "    with torch.no_grad():\n",
    "        for batch, (pr,dem,hr) in enumerate(val_dataloders):\n",
    "            pr,dem,hr = prepare([pr,dem, hr],device)\n",
    "            sr = net(pr,dem)\n",
    "            val_loss=criterion(sr, hr)\n",
    "            test_loss+=val_loss.item()\n",
    "            psnr = 10 * log10(1000 / (val_loss.item())**2)\n",
    "            avg_psnr += psnr\n",
    "        write_log(\"evalutaion: time cost %f s, test_loss: %f, psnr: avg_psnr %f\"%(\n",
    "                      time.time()-start,\n",
    "                      test_loss/(batch + 1),\n",
    "                      avg_psnr / len(val_dataloders)\n",
    "                 ))\n",
    "    return test_loss\n",
    "\n",
    "def prepare( l, device=False):\n",
    "    def _prepare(tensor):\n",
    "        if args.precision == 'half': tensor = tensor.half()\n",
    "        if args.precision == 'single': tensor = tensor.float()\n",
    "        return tensor.to(device)\n",
    "\n",
    "    return [_prepare(_l) for _l in l]\n",
    "\n",
    "# find 50 satation # lat, lon\n",
    "# from util.constant.param import 50_station_index\n",
    "\n",
    "\n",
    "station_50_index={'CARNAMAH': [128, 28],\n",
    " 'MULLEWA': [138, 24],\n",
    " 'WONGAN HILLS': [117, 35],\n",
    " 'BADGINGARRA RESEARCH STN': [122, 24],\n",
    " 'MOUNT BARKER': [83, 44],\n",
    " 'ESPERANCE': [90, 82],\n",
    " 'BENCUBBIN': [118, 46],\n",
    " 'KELLERBERRIN': [110, 44],\n",
    " 'BEVERLEY': [106, 37],\n",
    " 'CORRIGIN': [104, 46],\n",
    " 'HYDEN': [103, 55],\n",
    " 'NARROGIN': [99, 39],\n",
    " 'ONGERUP': [89, 51],\n",
    " 'RAVENSTHORPE': [93, 65],\n",
    " 'SALMON GUMS RES.STN.': [98, 80],\n",
    " 'CEDUNA AMO': [106, 190],\n",
    " 'CLEVE': [92, 215],\n",
    " 'KYANCUTTA': [97, 206],\n",
    " 'STREAKY BAY': [100, 194],\n",
    " 'YONGALA': [98, 236],\n",
    " 'PRICE': [86, 229],\n",
    " 'WAROOKA': [80, 223],\n",
    " 'ROSEDALE (TURRETFIELD RESEARCH CENTRE)': [84, 236],\n",
    " 'MENINGIE': [73, 241],\n",
    " 'KEITH': [70, 250],\n",
    " 'SPRINGSURE COMET ST': [179, 320],\n",
    " 'OAKEY AERO': [149, 354],\n",
    " 'SURAT': [151, 329],\n",
    " 'LAKE VICTORIA STORAGE': [88, 258],\n",
    " 'COLLARENEBRI (ALBERT ST)': [129, 325],\n",
    " 'BALRANALD (RSL)': [83, 279],\n",
    " 'PEAK HILL POST OFFICE': [100, 321],\n",
    " 'CONDOBOLIN AG RESEARCH STN': [97, 313],\n",
    " 'NYNGAN AIRPORT': [111, 312],\n",
    " 'TRANGIE RESEARCH STATION AWS': [107, 319],\n",
    " 'MUNGINDI POST OFFICE': [134, 329],\n",
    " 'QUIRINDI POST OFFICE': [111, 344],\n",
    " 'ORANGE AGRICULTURAL INSTITUTE': [95, 329],\n",
    " 'WAGGA WAGGA AMO': [78, 315],\n",
    " 'GRENFELL (MANGANESE RD)': [90, 321],\n",
    " 'COROWA AIRPORT': [71, 305],\n",
    " 'NARRANDERA AIRPORT AWS': [82, 306],\n",
    " 'HILLSTON AIRPORT': [93, 297],\n",
    " 'LAKE CARGELLIGO AIRPORT': [95, 305],\n",
    " 'MILDURA AIRPORT': [87, 266],\n",
    " 'OUYEN (POST OFFICE)': [79, 268],\n",
    " 'WARRACKNABEAL MUSEUM': [68, 269],\n",
    " 'ECHUCA AERODROME': [69, 290],\n",
    " 'KERANG': [73, 282],\n",
    " 'ARARAT PRISON': [59, 274]}\n",
    "\n",
    "\n",
    "def crps(ensin,obs):\n",
    "    '''\n",
    "    @param ensin A vector of prediction\n",
    "    @param obs  A vector of observations\n",
    "    \n",
    "'''\n",
    "\n",
    "#     assert not np.isnan(ensin).any() and not np.isnan(obs).any(), \"data contains nan\"\n",
    "         \n",
    "    Fn = ECDF(ensin)\n",
    "    xn=np.sort(np.unique(ensin))\n",
    "    m=len(xn)\n",
    "    dn=np.diff(xn)\n",
    "    eq1=0\n",
    "    eq2=0\n",
    "    if(obs>xn[0] and obs<xn[m-1]): #obsåœ¨èŒƒå›´å†…\n",
    "        k=np.max(np.where(xn<=obs))#å°äºŽobsçš„æœ€å¤§å€¼ä¸‹æ ‡\n",
    "        x0 = xn[k] #å°äºŽobsçš„æœ€å¤§å€¼\n",
    "        if k>0:\n",
    "            eq1=np.sum(Fn(xn[0:k+1])**2*np.append(dn[0:k], obs - xn[k]))#å°äºŽobsçš„æ‰€æœ‰å€¼ çš„ ç™¾åˆ†æ¯”æ•° çš„å¹³æ–¹\n",
    "        else:\n",
    "            eq1 =np.sum(Fn(xn[0])**2*(obs - xn[0]))\n",
    "        if k<m-2:\n",
    "\n",
    "            eq2=np.sum((1-Fn(xn[k:m-1]))**2*np.append(xn[k+1] - obs, dn[(k+1):(m-1)]))\n",
    "        else:\n",
    "            eq2 =np.sum((1-Fn(xn[m-2]))**2*(xn[m-1] - obs))\n",
    "\n",
    "    if obs <= xn[0]: # è§‚æµ‹å€¼åœ¨ä¹‹å¤–\n",
    "        eq2 =np.sum(np.append(1, 1-Fn(xn[0:(m-1)]))**2*np.append(xn[0]-obs, dn))\n",
    "    if obs >= xn[m-1]:\n",
    "        eq1= np.sum(Fn(xn)**2*np.append(dn, obs - xn[m-1]))\n",
    "            \n",
    "    return eq1+eq2 \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def vectcrps_v(fct_ens,obs):\n",
    "    '''\n",
    "    #' @param fct_ens A 2D prediction\n",
    "    #' @param obs  A vector of observations\n",
    "    #' @return a crps vector'''\n",
    "    score =0\n",
    "\n",
    "    \n",
    "    fct_ens=fct_ens\n",
    "    assert not np.isnan(fct_ens).any() and not np.isnan(obs).any(),\"data contains nan\"\n",
    "    for i in range(obs.shape[0]):\n",
    "#         print(fct_ens[:,i],obs[i])\n",
    "        score+=crps(fct_ens[:,i],obs[i])\n",
    "  \n",
    "    return score\n",
    "\n",
    "\n",
    "def vectcrps_m(fct_ens,obs):\n",
    "    '''\n",
    "    #' @param fct_ens A 2D prediction\n",
    "    #' @param obs  A vector of observations\n",
    "    #' @return a crps vector'''\n",
    "    score =0\n",
    "    assert not np.isnan(fct_ens).any() and not np.isnan(obs).any(),\"data contains nan\"\n",
    "    score_map=np.zeros((obs.shape[0],obs.shape[1]))\n",
    "    for i in range(obs.shape[0]):\n",
    "        for j in range(obs.shape[1]):\n",
    "            score_map[i,j]=crps(fct_ens[:,i,j],obs[i,j])\n",
    "#             score+=crps(fct_ens[:,i,j],obs[i,j])\n",
    "    return score_map\n",
    "    return score/(obs.shape[0]*obs.shape[1])\n",
    "\n",
    "def vectcrps_m_50_station(fct_ens,obs):\n",
    "    '''\n",
    "    #' @param fct_ens A 2D prediction\n",
    "    #' @param obs  A vector of observations\n",
    "    #' @return a crps vector'''\n",
    "    score =0\n",
    "    assert not np.isnan(fct_ens).any() and not np.isnan(obs).any(),\"data contains nan\"\n",
    "    score_map=np.zeros((50))\n",
    "    for idx,i in enumerate(station_50_index.values()):\n",
    "        score_map[idx]=crps(fct_ens[:,i[0],i[1]],obs[i[0],i[1]])\n",
    "#     for i in range(obs.shape[0]):\n",
    "#         for j in range(obs.shape[1]):\n",
    "#             score_map[i,j]=crps(fct_ens[:,i,j],obs[i,j])\n",
    "#             score+=crps(fct_ens[:,i,j],obs[i,j])\n",
    "    return score_map\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    #     init_date=date(1970, 1, 1)\n",
    "    #     start_date=date(1990, 1, 2)\n",
    "    #     end_date=date(2011,12,25)\n",
    "    sys = platform.system()\n",
    "    args.dem=True\n",
    "    args.train_name=\"pr_dem\"\n",
    "    args.channels=0\n",
    "    if args.pr:\n",
    "        args.channels+=1\n",
    "    if args.zg:\n",
    "        args.channels+=1\n",
    "    if args.psl:\n",
    "        args.channels+=1\n",
    "    if args.tasmax:\n",
    "        args.channels+=1\n",
    "    if args.tasmin:\n",
    "        args.channels+=1\n",
    "    if args.dem:\n",
    "        args.channels+=1\n",
    "    print(\"training statistics:\")\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  trainning name  |  %s\"%args.train_name)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  num of channels | %5d\"%args.channels)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  num of threads  | %5d\"%args.n_threads)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  batch_size     | %5d\"%args.batch_size)\n",
    "    print(\"  ------------------------------\")\n",
    "    print(\"  using cpu only | %5d\"%args.cpu)\n",
    "\n",
    "    lr_transforms = transforms.Compose([\n",
    "        transforms.Resize((316, 376)),\n",
    "    #     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    #     transforms.RandomHorizontalFlip(),\n",
    "    #     transforms.RandomRotation(30),\n",
    "        transforms.ToTensor()\n",
    "    #     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "\n",
    "    hr_transforms = transforms.Compose([\n",
    "    #         transforms.Resize((316, 376)),\n",
    "    #     transforms.RandomResizedCrop(IMG_SIZE),\n",
    "    #     transforms.RandomHorizontalFlip(),\n",
    "    #     transforms.RandomRotation(30),\n",
    "        transforms.ToTensor()\n",
    "    #     transforms.Normalize(IMG_MEAN, IMG_STD)\n",
    "    ])\n",
    "\n",
    "    data_set=ACCESS_BARRA_vdsr_pr_dem(args.test_start_time,args.test_end_time,lr_transform=lr_transforms,hr_transform=hr_transforms,shuffle=False,args=args)\n",
    "\n",
    "\n",
    "\n",
    "    #     #######################################################################\n",
    "\n",
    "    test_data=DataLoader(data_set,\n",
    "                                            batch_size=args.batch_size,\n",
    "                                            shuffle=False,\n",
    "                                num_workers=args.n_threads,drop_last=True)\n",
    "\n",
    "    #     #######################################################################\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # net=torch.load(\"./save/vdsr_pr/best_test.pth\")\n",
    "    # net=torch.load(\"../data/model/vdsr_pr/best_test.pth\")\n",
    "    net=torch.load(\"./save/vdsr_pr/best_test.pth\",map_location=torch.device('cpu'))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # #     criterion = nn.MSELoss(size_average=False)\n",
    "    #     criterion=nn.L1Loss()\n",
    "\n",
    "    #     optimizer = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9, weight_decay=1e-4)\n",
    "    # #     optimizer_my = optim.SGD(net.parameters(), lr=args.lr, momentum=0.9)\n",
    "    #     scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)\n",
    "    if torch.cuda.device_count() > 1:\n",
    "        write_log(\"!!!!!!!!!!!!!Let's use\"+str(torch.cuda.device_count())+\"GPUs!\")\n",
    "        net = nn.DataParallel(net,range(torch.cuda.device_count()))\n",
    "    else:\n",
    "        write_log(\"Let's use\"+str(torch.cuda.device_count())+\"GPUs!\")\n",
    "\n",
    "    net.to(device)\n",
    "    #     ##############################################\n",
    "    write_log(\"start\")\n",
    "    #     max_error=np.inf\n",
    "    #     val_max_error=np.inf\n",
    "\n",
    "    #     print(data_set.filename_list)\n",
    "\n",
    "    # for e in range(args.nEpochs):\n",
    "    #         loss=0\n",
    "    for lead in range(1,31):\n",
    "        args.leading_time_we_use=lead\n",
    "\n",
    "        data_set=ACCESS_BARRA_vdsr_pr_dem(args.test_start_time,args.test_end_time,lr_transform=lr_transforms,hr_transform=hr_transforms,shuffle=False,args=args)\n",
    "\n",
    "\n",
    "        test_data=DataLoader(data_set,\n",
    "                                                batch_size=args.batch_size,\n",
    "                                                shuffle=False,\n",
    "                                    num_workers=args.n_threads,drop_last=False)\n",
    "\n",
    "\n",
    "        crps_score_vsdr=[]\n",
    "        start=time.time()\n",
    "        fmt = '%Y%m%d'\n",
    "\n",
    "\n",
    "        test_data=tqdm.tqdm(test_data)\n",
    "        for batch, (pr,dem,hr,en,data_time,idx) in enumerate(test_data):\n",
    "\n",
    "            pr,dem,hr= prepare([pr,dem,hr],device)\n",
    "        #             print(en,data_time,idx)\n",
    "        #             optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(False):\n",
    "\n",
    "                sr = net(pr)\n",
    "                sr_np=sr.cpu().numpy()\n",
    "                hr_np=hr.cpu().numpy()\n",
    "                for i in range(args.batch_size//args.ensemble):\n",
    "                    a=np.squeeze( sr_np[i*args.ensemble:(i+1)*args.ensemble])\n",
    "                    b=np.squeeze(hr_np[i*args.ensemble])\n",
    "        #             print(a.shape,b.shape)\n",
    "        #             skil=vectcrps_m(a,b)\n",
    "                    skil=vectcrps_m_50_station(a,b)\n",
    "    #                 print(skil.shape)\n",
    "                    time_tuple = time.strptime(str(data_time[i*args.ensemble].item()), fmt)\n",
    "                    year, month, day = time_tuple[:3]\n",
    "                    a_date = date(year, month, day)\n",
    "    #                 print(data_time,idx)\n",
    "        #             np.save(\"../crps/vsdr_pr/\"+(a_date+timedelta(idx[i*args.ensemble].item())).strftime(\"%Y%m%d\")+'_50station',skil)\n",
    "                    crps_score_vsdr.append(skil)\n",
    "        np.save(\"./save/crps/50_station_pr_dem_model/lead_time\"+str(lead)+'_50station',crps_score_vsdr)\n",
    "        print(str(lead)+\" : \"+str(np.array(crps_score_vsdr).mean()))\n",
    "\n",
    "            \n",
    "if __name__=='__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
